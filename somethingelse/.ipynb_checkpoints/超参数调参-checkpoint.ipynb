{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参数调优方法\n",
    "超参数搜索算法一般包含以下几个要输：**目标函数**，即算法需要最大化/最小化的目标，**搜索范围**,一般通过上限和下限决定，算法的其他参数，例如**步长**。\n",
    "## 网格搜素\n",
    "网格搜索是最简单、应用最广泛的超参数搜索算法，**通过查找搜索范围内所有的点来确定最优值**。如果采用较大的搜索范围以及较小的步长，网格搜索会有很大的概率找到全局最优值。然而该算法缺点也明显：如果超参数比较多，会消耗巨大的算力和时间。因此在实际操作中，一般先适用较广的搜索范围和较大的步长来寻找全局最优点的大致位置。然后逐步缩小搜索范围和步长，来寻找更精确的最优值。明确一点：由于目标函数一般是非凸的，很可能错过全局最优值。\n",
    "## 随机搜索\n",
    "随机搜索跟网格搜索类似，只是不再测试上界和下界之间的所有值，而是在搜索范围内随机选取样本点。理论依据是：如果样本点集足够大，那么通过随机采样也能大概率地找到全局最优点。显然该做法结果无法保证。\n",
    "## 贝叶斯优化算法\n",
    "网格搜索和随机搜索在测试一个新点时，会忽略前一个点的信息，而贝叶斯优化算法则充分利用之前的信息。贝叶斯优化算法通过对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。具体而言，它学习目标函数形状的方法是，首先根据先验分布，假设一个搜集函数，然后每一次适用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布，最后算法测试由后验分布给出全局最值最可能出现的位置的点。值得注意的是，当贝叶斯优化算法找到一个局部最优值，它会在该区域不断采样，所有很容易陷入局部最值。为弥补该缺陷，算法通常会在探索（在未取样的区域取点）和利用（根据后验分布在最可能出现最值的点采样）之间找一个平衡点。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "默认的kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
